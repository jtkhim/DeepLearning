## DNN Basics ##

() Feedforward networks
    * notation / definitions / big questions
    * basic training
      - backprop, mini-batch gradient descent
() Tensorflow: a general purpose optimzer
    * high level overview, regression example, MNIST example
    * HoML Ch. 9 / 10
() Tricks of the trade
    * issues and strategies for training DNN [1,2]
      - regularization, dropout, batch normalization, faster optimizers
    * HoML Ch. 11

## Special Topics ##

() Autoencoders: a framework for dimensionality reduction
    * HoML Ch. 15
() Convnets: architectures for image classification
    * HoML Ch. 13
() Recurrent NN: a special architecture for time dependent data
    * HoML Ch. 14
() Reinforcement Learning [3]
    * http://karpathy.github.io/2016/05/31/rl/
    * HoML Ch. 16

## Possible Presentation Topics ##

() Word2Vec
() AlphaGo
() ...

## Materials ##

Hands on Machine Learning with Scikitlearn and Tensorflow (HoML)
    - http://shop.oreilly.com/product/0636920052289.do
    - https://github.com/ageron/handson-ml

[1] Bengio, Yoshua. Practical recommendations for gradient-based training of deep architectures.
[2] Wager, S., et al.  Dropout Training as Adaptive Regularization
[3] Arulkumaran, K., et al. A Brief Survery of Deep Reinforcement Learning.
[4] Zhang, C. Understanding Deep Learning Requires Rethinking Generalization.
